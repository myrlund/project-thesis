% Chapter 5

\chapter{Evaluation} % Main chapter title

\label{Chapter5}

\lhead{Chapter 5. \emph{Evaluation}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

% \emph{Beskriv hvordan du vurderer arbeidet ditt. Oppsummer evalueringsresultatene, og bruk dem til å vurdere ditt eget arbeid kritisk. Vær ærlig om eventuelle mangler. Hva betyr resultatene?}

\begin{itemize}
  \item When rating popular and well-known movies\footnote{The sample in question consisted of ``Pulp Fiction'', ``The Shining'', ``Mission: Impossible'', ``The Matrix'', ``The Godfather'', ``Forrest Gump'', and ``A Clockwork Orange''.} the predicted ratings achieve a correlational coefficient of 0.75 with regard to average Netflix ratings for the same movie.
  \item B-movies or older less-known movies rarely collect enough Twitter search results to warrant any further analysis.
  \item Movies with titles that are fairly common words or expressions in their own right achieve very low precision, and often return only noise. This is hard to detect without manual interference. Need to perform some sort of Named Entity Disambiguation, maybe something like the techniques outlined in Cucerzan~\cite{NamedEntityDisambiguationWiki} or Sarmento~\cite{NamedEntityDisambiguationWS} (is elaboration needed?).
\end{itemize}

\section{Evaluation metrics} % (fold)
\label{sec:evaluation_metrics}

To find out how the Twitter-based predictions fare, we will simply use the average of the available Netflix ratings of the same titles as benchmark ratings.

To compute error, the MAE (Mean Average Error) metric will be used. With predictions $p$ and benchmark ratings $r$, we will compute the MAE of $N$ sample movies in the following way:

\begin{equation}
  \text{MAE} = \frac{1}{N} \sum_{i=1}^N |p_i - r_i|
  \label{eq:mae}
\end{equation}

As a baseline metric, the MAE of the average of all the benchmark ratings is used:

\begin{align}
  \text{MAE}_\text{baseline} = \sqrt{ \frac{1}{N} \sum_{i=1}^N (\bar{r} - r_i)^2 }
\end{align}

To measure the correlation between predictions and ratings, Pearson's $r$ is used. It is calculated in the following way:

\begin{equation}
  \rho_{X,Y} = \frac{\cov(X, Y)}{\sigma_X \sigma_Y}
\end{equation}

\subsection{Why MAE?} % (fold)
\label{sub:why_mae}

Two error metrics were considered for evaluating the system: MAE and RMSE. We specifically consider MAE because of its simplicity, and RMSE because of its widespread use in the domain of evaluating movie recommendation algorithms\footnote{It was the only error metric targeted by the Netflix Prize, whence the Netflix training data originate.}.

First, let's have a look at RMSE. It is calculated in the following manner for $N$ predictions $p$ and benchmark ratings $r$:

\begin{equation}
  \text{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^N (p_i - r_i)^2 }
\end{equation}

The main downside to RMSE is that it is complex. It varies with three different qualities of the errors, namely

\begin{enumerate}
  \item the variability within the distribution of error magnitudes
  \item the square root of the number of errors
  \item the average error magnitude (MAE)
\end{enumerate}

The MAE, on the other hand -- as defined in~\eqref{eq:mae} -- is a lot simpler.

The goal for the evaluation of this application is to enable us to reason over the applicability of the general approach taken, not to objectively compare this specific application to other competing systems. For this, the simplicity and clarity of MAE is prioritized over RMSE's expressiveness, and will serve as the main error metric.

\section{Comparison of results from different movie types} % (fold)
\label{sec:comparison_of_results_from_different_movie_types}

Let us have a look at two sample runs. The first one selects a sample of 10 random movies from the entire set of movies in the test set, while the second run is restricted to movies present in IMDB's top 250 list (as described in section~\ref{sec:evaluation_impl}).

The results shown below are consistent with the magnitude of those gathered from evaluating other corresponding samples.

Both runs were performed with the same system parameters, apart from the top movie constraint in the second run, the most important ones listed in table~\ref{tab:run_parameters}.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{ll}
      \textbf{Max. document count} & At most 25 Twitter documents were analyzed per title. \\
      \textbf{Threshold}           & 5 Twitter messages required to start sentiment analysis. \\
      \textbf{Result type}         & ``Mixed'' \\
    \end{tabular}
  \end{center}
  \caption{Parameters for evaluated test runs.}
  \label{tab:run_parameters}
\end{table}

\subsection{Run 1: Sample drawn from entire test set}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|rr|r|}
      \hline
      \textbf{Title} & \textbf{Prediction} & \textbf{Variance} & \textbf{Benchmark} \\
      \hline
      Chill Out          & 4.28 & 1.37 & 2.49 \\
      First Kid          & 4.48 & 1.21 & 3.59 \\
      Paparazzi          & 3.75 & 1.39 & 3.39 \\
      That Thing You Do! & 4.80 & 0.60 & 3.53 \\
      Undercover Brother & 4.05 & 1.59 & 3.10 \\
      The Backyard       & 4.47 & 1.36 & 2.36 \\
      Black Cat          & 3.80 & 1.70 & 2.30 \\
      Dangerous Game     & 3.83 & 1.52 & 2.54 \\
      So Little Time     & 3.60 & 1.80 & 3.10 \\
      Play for Me        & 2.60 & 1.96 & 2.42 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Test run for a sample of movies randomly selected from the entire test set.}
  \label{tab:test_run_all}
\end{table}

This run gives a MAE of 1.08, and a correlational coefficient of 0.39. However, these numbers vary greatly between each run. Subsequent runs with other randomly sampled movies yield the following:

\begin{enumerate}
  \item MAE: 1.16, correlational coefficient: 0.16
  \item MAE: 0.62, correlational coefficient: -0.56
  \item MAE: 0.84, correlational coefficient: -0.11
\end{enumerate}

Specifically, it is worth noting that the predictions in general grossly overshoot the benchmarks, which becomes especially clear in the comparative line plot in figure~\ref{fig:predictions_benchmark_rand}.

For the first run, it is worth pointing out that 7 movie titles were discarded for not meeting the required threshold of 5 Twitter search results. On average, approximately 50\% of the titles are discarded on this criteria.

\begin{figure}[h]
  \centering
    \includegraphics[width=.8\textwidth]{Figures/plots/predictions_benchmark_rand}
  \caption{Run 1: Line plot of predictions vs. benchmark value.}
  \label{fig:predictions_benchmark_rand}
\end{figure}

\subsubsection{Noisy Twitter search results}

Most of the Twitter results listed in table~\ref{tab:test_run_all} were in fact not about the movie in question. In fact, \emph{all} the search results that were evaluated on behalf of the first movie, ``Chill Out'', were about watching other movies, only mentioning ``chill out'' in the same message more or less at random.

The same is the case for several other movies in the listed sample run.

\subsection{Run 2: Sample drawn from IMDB's top 250 list}

\begin{table}[h]
  \begin{center}
    \begin{tabular}{|l|rr|r|}
      \hline
      \textbf{Title} & \textbf{Prediction} & \textbf{Variance} & \textbf{Benchmark} \\
      \hline
      The Kid             & 3.96 & 1.57 & 3.32 \\
      Metropolis          & 3.48 & 1.17 & 3.40 \\
      Reservoir Dogs      & 4.28 & 1.11 & 4.00 \\
      The Usual Suspects  & 4.04 & 1.51 & 4.37 \\
      Monsters, Inc.      & 3.64 & 1.76 & 4.28 \\
      A Beautiful Mind    & 4.44 & 1.33 & 3.97 \\
      Requiem for a Dream & 3.08 & 2.00 & 3.79 \\
      Million Dollar Baby & 4.20 & 1.60 & 4.16 \\
      City of God         & 4.76 & 0.11 & 4.10 \\
      Jurassic Park       & 3.80 & 1.60 & 3.80 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Test run for a sample of movies randomly selected from IMDB's top 250 list.}
  \label{tab:test_run_popular}
\end{table}

This run gives a MAE of 0.38, and a correlational coefficient of 0.40.

Subsequent runs with the same parameters yield the following:

\begin{enumerate}
  \item MAE: 0.78, correlational coefficient: 0.16
  \item MAE: 0.58, correlational coefficient: -0.27
  \item MAE: 0.52, correlational coefficient: 0.23
\end{enumerate}

The comparative plot in figure~\ref{fig:predictions_benchmark_pop} is better adjusted with regard to overshooting the benchmark values. However, the plot fails to emphasize is that the degree of correlation is approximately the same in both runs.

\begin{figure}[h]
  \centering
    \includegraphics[width=.8\textwidth]{Figures/plots/predictions_benchmark_pop}
  \caption{Run 2: Line plot of predictions vs. benchmark value.}
  \label{fig:predictions_benchmark_pop}
\end{figure}

\subsection{Evaluation result comparison} % (fold)
\label{sub:evaluation_result_comparison}

Up to this point, the predictions have only been compared with the benchmarks. Here, they will be compared to each other.

First, the benchmarks. Let the set of benchmark ratings for run 1 be denoted $R_1$, and similarly the benchmark ratings from run 2 $R_2$. As expected, the average benchmark ratings from run 2 -- the top movies -- are clearly above the ones from run 1.

\begin{align}
  \bar{R}_1 = 2.882 \\
  \bar{R}_2 = 3.919
\end{align}

A difference of more than $1.0$. A number along these lines is what we want to see as the difference between the average predictions from the two runs.

However, when we perform the same mean value comparison for the \emph{predictions} from the two runs, denoted $P_1$ and $P_2$, we see a different pattern:

\begin{align}
  \bar{P}_1 = 3.966 \\
  \bar{P}_2 = 3.968
\end{align}

A difference of only 0.002 -- a mere nothing.

This result repeats in approximately the same way for every run: the average rating predictions seem to be completely disassociated with the benchmark ratings of the test set.

% subsection evaluation_run_summary (end)
